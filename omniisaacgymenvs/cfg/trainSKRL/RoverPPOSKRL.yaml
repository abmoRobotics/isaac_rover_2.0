
network:
  mlp: 
    layers: [256,160,128]
    activation: leakyrelu

  encoder:
    layers: [80,60]
    activation: leakyrelu
    
config:
  rollouts: 30
  learning_epochs: 4
  mini_batches: 30
  discount_factor: 0.99
  lambda: 0.95

  learning_rate: 1e-4

  grad_norm_clip: 1.0
  ratio_clip: 0.2
  value_clip: 0.2
  clip_predicted_values: True

  entropy_loss_scale: 0.0
  value_loss_scale: 1.0

  kl_threshold: 0.008

experiment:
  write_interval: 50
  checkpoint_interval: 100
  timesteps: 50000
  headless: True

